{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "df_read = pd.read_parquet('CL_fr-en (1).parquet', engine='pyarrow')\n",
    "\n",
    "df_read = df_read.dropna()\n",
    "\n",
    "df_read['fr'] = df_read['fr'].str.lower()\n",
    "df_read['en'] = df_read['en'].str.lower()\n",
    "\n",
    "df_read['fr'] = df_read['fr'].str.replace('[^\\w\\s]','')\n",
    "df_read['en'] = df_read['en'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "\n",
    "#remove punctuation\n",
    "import string\n",
    "df_read['fr'] = df_read['fr'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "df_read['en'] = df_read['en'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "df_read=df_read.head(1000)\n",
    "concatdf = pd.concat([df_read['en'], df_read['fr']], ignore_index=True)\n",
    "#save concatdf to a txt file without quotes\n",
    "concatdf.to_csv('concatdf.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, stopwords_file, training_text_file):\n",
    "        self.stopwords = self._load_stopwords(stopwords_file)\n",
    "        self.corpus = self._process_text(training_text_file)\n",
    "    \n",
    "    def _load_stopwords(self, stopwords_file):\n",
    "        with open(stopwords_file) as f:\n",
    "            stopwords = f.read().replace('\\n', ' ').split()\n",
    "        return stopwords\n",
    "\n",
    "    def _process_text(self, training_text_file):\n",
    "        with open(training_text_file, encoding='utf-8') as f:\n",
    "            text = f.read().replace('\\n', '')\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            text = ''.join([t for t in text if t not in list('0123456789')])\n",
    "            text = text.replace('”', '').replace('“', '').replace('’', '').lower().split()\n",
    "        return [w for w in text if w not in self.stopwords][:2000]\n",
    "\n",
    "    def get_corpus(self):\n",
    "        return self.corpus\n",
    "\n",
    "\n",
    "class GensimWord2Vec:\n",
    "    def __init__(self, corpus, embed_size, window_size, learning_rate, epochs):\n",
    "        self.corpus = [corpus]\n",
    "        self.embed_size = embed_size\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self):\n",
    "        self.model = Word2Vec(sentences=self.corpus,\n",
    "                              vector_size=self.embed_size,\n",
    "                              window=self.window_size,\n",
    "                              alpha=self.learning_rate,\n",
    "                              epochs=self.epochs,\n",
    "                              sg=1,  # Use skip-gram\n",
    "                              workers=1)  # Single worker for reproducibility\n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        return self.model.wv[word]\n",
    "    \n",
    "    def find_closest_word(self, word):\n",
    "        max_similarity = -1.0  # Start with the lowest possible similarity value\n",
    "        closest_word = None\n",
    "\n",
    "        if word not in self.model.wv:  # Check if the word exists in the model\n",
    "            return f\"{word} not found in model\"\n",
    "\n",
    "        word_vector = np.array(self.model.wv[word]).reshape(1, -1)\n",
    "        for w in self.model.wv.index_to_key:  # Loop through the model's vocab\n",
    "            if w == word:  # Skip the word itself\n",
    "                continue\n",
    "            w_vector = np.array(self.model.wv[w]).reshape(1, -1)\n",
    "            similarity_output = cosine_similarity(word_vector, w_vector)\n",
    "\n",
    "            # Check the type of the output\n",
    "            if np.isscalar(similarity_output):\n",
    "                similarity = similarity_output\n",
    "            else:\n",
    "                similarity = similarity_output[0][0]\n",
    "\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                closest_word = w\n",
    "\n",
    "        return closest_word\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, target, context):\n",
    "        in_vectors = self.in_embed(target)\n",
    "        out_vectors = self.out_embed(context)\n",
    "        scores = torch.matmul(in_vectors, out_vectors.t())\n",
    "        return scores\n",
    "\n",
    "    def train_model(self, corpus, vocab, window_size, learning_rate, epochs, negative_samples=5):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for idx, word in enumerate(corpus):\n",
    "                # Get the context words based on window size\n",
    "                start = max(0, idx - window_size)\n",
    "                end = min(len(corpus), idx + window_size + 1)\n",
    "                context_words = corpus[start:idx] + corpus[idx+1:end]\n",
    "\n",
    "                for context_word in context_words:\n",
    "                    target_tensor = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "                    context_tensor = torch.tensor([vocab[context_word]], dtype=torch.long)\n",
    "                    \n",
    "                    negative_context = torch.multinomial(torch.tensor(list(vocab.values()), dtype=torch.float), negative_samples, replacement=True)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    scores = self(target_tensor, torch.cat([context_tensor, negative_context]))\n",
    "                    \n",
    "                    # Create labels: 1 for the correct context word and 0 for the negative samples\n",
    "                    labels = torch.cat([torch.tensor([1.0]), torch.zeros(negative_samples)]).unsqueeze(0)\n",
    "                    \n",
    "                    loss = loss_fn(scores, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    \n",
    "    def get_word_vector(self, word, vocab):\n",
    "        word_idx = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "        return self.in_embed(word_idx).squeeze().detach().numpy()\n",
    "    \n",
    "    def find_closest_word(self, word, vocab):\n",
    "            max_similarity = -1.0  # Start with the lowest possible similarity value\n",
    "            closest_word = None\n",
    "\n",
    "            if word not in vocab:  # Check if the word exists in the vocab\n",
    "                return f\"{word} not found in vocab\"\n",
    "\n",
    "            word_idx = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "            word_vector = self.in_embed(word_idx).detach().numpy().reshape(1, -1)\n",
    "\n",
    "            for w, idx in vocab.items():  # Loop through the vocab\n",
    "                if w == word:  # Skip the word itself\n",
    "                    continue\n",
    "                w_idx = torch.tensor([idx], dtype=torch.long)\n",
    "                w_vector = self.in_embed(w_idx).detach().numpy().reshape(1, -1)\n",
    "\n",
    "                similarity_output = cosine_similarity(word_vector, w_vector)\n",
    "                \n",
    "                # Check the type of the output\n",
    "                if np.isscalar(similarity_output):\n",
    "                    similarity = similarity_output\n",
    "                else:\n",
    "                    similarity = similarity_output[0][0]\n",
    "                \n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    closest_word = w\n",
    "\n",
    "            return closest_word\n",
    "    \n",
    "\n",
    "# Vocabulary builder\n",
    "def build_vocabulary(corpus):\n",
    "    vocab = {}\n",
    "    for word in corpus:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = vec1.squeeze()\n",
    "    vec2 = vec2.squeeze()\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'e'\n",
    "word2 = 'e'\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "example_word = 'data'\n",
    "embed_size = 20\n",
    "window_size = 2\n",
    "learning_rate = 0.025\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'learning', 'fundamentals', 'data', 'science', 'statistics', 'data', 'science', 'statistics', 'hot', 'growing', 'fields', 'alternative', 'names', 'machine', 'learning', 'artificial', 'intelligence', 'big', 'data', 'etc', 'im', 'really', 'excited', 'talk', 'data', 'science', 'statistics', 'data', 'science', 'statistics', 'long', 'passions', 'mine', 'didnt', 'used', 'good', 'data', 'science', 'statistics', 'studying', 'data', 'science', 'statistics', 'long', 'time', 'got', 'better', 'better', 'became', 'data', 'science', 'statistics', 'expert', 'im', 'really', 'excited', 'talk', 'data', 'science', 'statistics', 'thanks', 'listening', 'talk', 'data', 'science', 'statistics']\n",
      "<class 'list'>\n",
      "{'today': 0, 'learning': 1, 'fundamentals': 2, 'data': 3, 'science': 4, 'statistics': 5, 'hot': 6, 'growing': 7, 'fields': 8, 'alternative': 9, 'names': 10, 'machine': 11, 'artificial': 12, 'intelligence': 13, 'big': 14, 'etc': 15, 'im': 16, 'really': 17, 'excited': 18, 'talk': 19, 'long': 20, 'passions': 21, 'mine': 22, 'didnt': 23, 'used': 24, 'good': 25, 'studying': 26, 'time': 27, 'got': 28, 'better': 29, 'became': 30, 'expert': 31, 'thanks': 32, 'listening': 33}\n",
      "<class 'dict'>\n",
      "34\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and process the data\n",
    "processor = TextProcessor('stopwords.txt', 'training_text.txt')\n",
    "\n",
    "\n",
    "corpus = processor.get_corpus()\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = build_vocabulary(corpus)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "#print corpus dtype\n",
    "print(corpus)\n",
    "print(type(corpus))\n",
    "print(vocab)\n",
    "print(type(vocab))\n",
    "print(vocab_size)\n",
    "print(type(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim Model Training\n",
    "print(\"Training Gensim Model...\")\n",
    "modelGensim = GensimWord2Vec(corpus, embed_size, window_size, learning_rate, epochs)\n",
    "modelGensim.train()\n",
    "\n",
    "# Test the Gensim model (change 'example_word' to any word in your corpus)\n",
    "\n",
    "if example_word in vocab:\n",
    "    print(f\"Gensim Vector for {example_word}: \", modelGensim.get_word_vector(example_word))\n",
    "    vec1_gensim = modelGensim.get_word_vector(word1)\n",
    "    vec2_gensim = modelGensim.get_word_vector(word2)\n",
    "    similarity_gensim = cosine_similarity(vec1_gensim, vec2_gensim)\n",
    "    print(f\"Gensim cosine similarity between {word1} and {word2}: \", similarity_gensim)\n",
    "else:\n",
    "    print(f\"{example_word} not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkipGram Model Training\n",
    "print(\"\\nTraining SkipGram Model...\")\n",
    "modelSkipGram = SkipGram(vocab_size, embed_size)\n",
    "modelSkipGram.train_model(corpus, vocab, window_size, learning_rate, epochs)\n",
    "\n",
    "# Test the SkipGram model (change 'example_word' to any word in your corpus)\n",
    "if example_word in vocab:\n",
    "    print(f\"SkipGram Vector for {example_word}: \", modelSkipGram.get_word_vector(example_word, vocab))\n",
    "    vec1_skipgram = modelSkipGram.get_word_vector(word1, vocab)\n",
    "    vec2_skipgram = modelSkipGram.get_word_vector(word2, vocab)\n",
    "    similarity_skipgram = cosine_similarity(vec1_skipgram, vec2_skipgram)\n",
    "    print(f\"SkipGram cosine similarity between {word1} and {word2}: \", similarity_skipgram)\n",
    "else:\n",
    "    print(f\"{example_word} not in vocabulary.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest word to 'data' using Gensim: statistics\n",
      "Closest word to 'data' using SkipGram: thanks\n"
     ]
    }
   ],
   "source": [
    "closest_gensim = modelGensim.find_closest_word('data')\n",
    "print(f\"Closest word to 'data' using Gensim: {closest_gensim}\")\n",
    "\n",
    "closest_skipgram = modelSkipGram.find_closest_word('data', vocab)\n",
    "print(f\"Closest word to 'data' using SkipGram: {closest_skipgram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim cosine similarity between data and science:  0.9797871\n",
      "SkipGram cosine similarity between data and science:  0.25304967\n",
      "CBOW cosine similarity between data and science:  -0.015298413\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "word1 = 'data'\n",
    "word2 = 'science'\n",
    "\n",
    "if word1 in vocab and word2 in vocab:\n",
    "    # Gensim Model\n",
    "    vec1_gensim = modelGensim.get_word_vector(word1)\n",
    "    vec2_gensim = modelGensim.get_word_vector(word2)\n",
    "    similarity_gensim = cosine_similarity(vec1_gensim, vec2_gensim)\n",
    "    print(f\"Gensim cosine similarity between {word1} and {word2}: \", similarity_gensim)\n",
    "\n",
    "    # SkipGram Model\n",
    "    vec1_skipgram = modelSkipGram.get_word_vector(word1, vocab)\n",
    "    vec2_skipgram = modelSkipGram.get_word_vector(word2, vocab)\n",
    "    similarity_skipgram = cosine_similarity(vec1_skipgram, vec2_skipgram)\n",
    "    print(f\"SkipGram cosine similarity between {word1} and {word2}: \", similarity_skipgram)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
