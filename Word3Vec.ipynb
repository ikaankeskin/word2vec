{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data\n",
    "\n",
    "##### Today we will be learning about the fundamentals of data science and statistics. \n",
    "##### Data Science and statistics are hot and growing fields with alternative names of machine learning, artificial intelligence, big data, etc. \n",
    "##### I'm really excited to talk to you about data science and statistics because data science and statistics have long been a passions of mine. \n",
    "##### I didn't used to be very good at data science and statistics but after studying data science and statistics for a long time, I got better and better at it until I became a data science and statistics expert. \n",
    "##### I'm really excited to talk to you about data science and statistics, thanks for listening to me talk about data science and statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENSIM & SkipGram & CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, stopwords_file, training_text_file):\n",
    "        self.stopwords = self._load_stopwords(stopwords_file)\n",
    "        self.corpus = self._process_text(training_text_file)\n",
    "    \n",
    "    def _load_stopwords(self, stopwords_file):\n",
    "        with open(stopwords_file) as f:\n",
    "            stopwords = f.read().replace('\\n', ' ').split()\n",
    "        return stopwords\n",
    "\n",
    "    def _process_text(self, training_text_file):\n",
    "        with open(training_text_file, encoding='utf-8') as f:\n",
    "            text = f.read().replace('\\n', '')\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            text = ''.join([t for t in text if t not in list('0123456789')])\n",
    "            text = text.replace('”', '').replace('“', '').replace('’', '').lower().split()\n",
    "        return [w for w in text if w not in self.stopwords][:2000]\n",
    "\n",
    "    def get_corpus(self):\n",
    "        return self.corpus\n",
    "\n",
    "\n",
    "class GensimWord2Vec:\n",
    "    def __init__(self, corpus, embed_size, window_size, learning_rate, epochs):\n",
    "        self.corpus = [corpus]\n",
    "        self.embed_size = embed_size\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self):\n",
    "        self.model = Word2Vec(sentences=self.corpus,\n",
    "                              vector_size=self.embed_size,\n",
    "                              window=self.window_size,\n",
    "                              alpha=self.learning_rate,\n",
    "                              epochs=self.epochs,\n",
    "                              sg=1,  # Use skip-gram\n",
    "                              workers=1)  # Single worker for reproducibility\n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        return self.model.wv[word]\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.out_linear = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, target):\n",
    "        in_vectors = self.in_embed(target)\n",
    "        out_logits = self.out_linear(in_vectors)\n",
    "        return out_logits\n",
    "    \n",
    "    def train_model(self, corpus, vocab, learning_rate, epochs):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for idx, word in enumerate(corpus[:-1]):  # -1 to avoid out of bounds\n",
    "                target = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "                context_idx = vocab[corpus[idx + 1]]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                predictions = self(target)\n",
    "                \n",
    "                loss = loss_fn(predictions, torch.tensor([context_idx]))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    def get_word_vector(self, word, vocab):\n",
    "        word_idx = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "        return self.in_embed(word_idx).detach().numpy()\n",
    "    \n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, context, window_size):\n",
    "        # Get the embeddings of the context words\n",
    "        in_vectors = self.in_embed(context)\n",
    "        \n",
    "        # Average the embeddings of the context words\n",
    "        in_vectors_avg = torch.mean(in_vectors, dim=0, keepdim=True)\n",
    "        \n",
    "        # Pass the averaged embeddings through the linear layer\n",
    "        out_logits = self.out_linear(in_vectors_avg)\n",
    "        return out_logits\n",
    "\n",
    "    def train_model(self, corpus, vocab, learning_rate, epochs, window_size=2):\n",
    "        self.out_linear = nn.Linear(self.embed_size, len(vocab)) \n",
    "        \n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for idx, word in enumerate(corpus[window_size:-window_size]):  # Leave space for context words\n",
    "                # Gather context words based on the window size\n",
    "                context_words = corpus[idx-window_size:idx] + corpus[idx+1:idx+1+window_size]\n",
    "                context_indices = [vocab[w] for w in context_words]\n",
    "                \n",
    "                # Convert to tensor\n",
    "                context = torch.tensor(context_indices, dtype=torch.long)\n",
    "                target_idx = vocab[word]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                predictions = self(context, window_size)\n",
    "                \n",
    "                loss = loss_fn(predictions, torch.tensor([target_idx]))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def get_word_vector(self, word, vocab):\n",
    "        word_idx = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "        return self.in_embed(word_idx).detach().numpy()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Vocabulary builder\n",
    "def build_vocabulary(corpus):\n",
    "    vocab = {}\n",
    "    for word in corpus:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gensim Model...\n",
      "Gensim Vector for data:  [-0.03911165  0.22466028  0.15663825  0.23718707 -0.17009805 -0.32686406\n",
      "  0.04607087  0.62932575 -0.24616416 -0.03631771 -0.08447191  0.0153377\n",
      " -0.01536816  0.06978628 -0.05106831 -0.19504882  0.37443215 -0.2380562\n",
      " -0.35111517 -0.24540682]\n",
      "\n",
      "Training SkipGram Model...\n",
      "SkipGram Vector for data:  [[-0.05340152  1.3186854  -0.99223226  0.8127032   1.1575003   0.49402523\n",
      "   1.6774234   0.19818336  0.08226159 -0.268004   -0.8270626   1.0272753\n",
      "  -0.01924454  0.49821365  0.0542079   0.09707388  0.55477446 -0.53761685\n",
      "  -1.623687   -0.7710329 ]]\n",
      "\n",
      "Training CBOW Model...\n",
      "CBOW Vector for data:  [[-1.7172759  -1.2148739  -0.21443495 -0.59840095  0.21292756  1.4785212\n",
      "  -1.8608963  -2.6497774   0.11688966  0.33669746 -1.4511781   4.2494607\n",
      "   0.22541954  2.0907726   0.53882873  3.0997822  -0.80189586 -2.754468\n",
      "   0.9497787  -0.35829023]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and process the data\n",
    "processor = TextProcessor('stopwords.txt', 'training_text.txt')\n",
    "corpus = processor.get_corpus()\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = build_vocabulary(corpus)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Gensim Model\n",
    "print(\"Training Gensim Model...\")\n",
    "modelGensim = GensimWord2Vec(corpus, 20, 2, 0.025, 1000)\n",
    "modelGensim.train()\n",
    "\n",
    "# Test the Gensim model (change 'example_word' to any word in your corpus)\n",
    "example_word = \"data\"\n",
    "if example_word in vocab:\n",
    "    print(f\"Gensim Vector for {example_word}: \", modelGensim.get_word_vector(example_word))\n",
    "else:\n",
    "    print(f\"{example_word} not in vocabulary.\")\n",
    "\n",
    "# SkipGram Model\n",
    "print(\"\\nTraining SkipGram Model...\")\n",
    "modelSkipGram = SkipGram(vocab_size, 20)\n",
    "modelSkipGram.train_model(corpus, vocab, 0.025, 1000)\n",
    "\n",
    "# Test the SkipGram model (change 'example_word' to any word in your corpus)\n",
    "if example_word in vocab:\n",
    "    print(f\"SkipGram Vector for {example_word}: \", modelSkipGram.get_word_vector(example_word, vocab))\n",
    "else:\n",
    "    print(f\"{example_word} not in vocabulary.\")\n",
    "\n",
    "# CBOW Model\n",
    "print(\"\\nTraining CBOW Model...\")\n",
    "modelCBOW = CBOW(vocab_size, 20)\n",
    "modelCBOW.train_model(corpus, vocab, 0.025, 1000, window_size=2)\n",
    "\n",
    "\n",
    "\n",
    "# Test the CBOW model (change 'example_word' to any word in your corpus)\n",
    "if example_word in vocab:\n",
    "    print(f\"CBOW Vector for {example_word}: \", modelCBOW.get_word_vector(example_word, vocab))\n",
    "else:\n",
    "    print(f\"{example_word} not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim cosine similarity between data and science:  0.9797871\n",
      "SkipGram cosine similarity between data and science:  0.097108334\n",
      "CBOW cosine similarity between data and science:  -0.10008272\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = vec1.squeeze()\n",
    "    vec2 = vec2.squeeze()\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "\n",
    "word1 = 'data'\n",
    "word2 = 'science'\n",
    "\n",
    "if word1 in vocab and word2 in vocab:\n",
    "    # Gensim Model\n",
    "    vec1_gensim = modelGensim.get_word_vector(word1)\n",
    "    vec2_gensim = modelGensim.get_word_vector(word2)\n",
    "    similarity_gensim = cosine_similarity(vec1_gensim, vec2_gensim)\n",
    "    print(f\"Gensim cosine similarity between {word1} and {word2}: \", similarity_gensim)\n",
    "\n",
    "    # SkipGram Model\n",
    "    vec1_skipgram = modelSkipGram.get_word_vector(word1, vocab)\n",
    "    vec2_skipgram = modelSkipGram.get_word_vector(word2, vocab)\n",
    "    similarity_skipgram = cosine_similarity(vec1_skipgram, vec2_skipgram)\n",
    "    print(f\"SkipGram cosine similarity between {word1} and {word2}: \", similarity_skipgram)\n",
    "\n",
    "    # CBOW Model\n",
    "    vec1_cbow = modelCBOW.get_word_vector(word1, vocab)\n",
    "    vec2_cbow = modelCBOW.get_word_vector(word2, vocab)\n",
    "    similarity_cbow = cosine_similarity(vec1_cbow, vec2_cbow)\n",
    "    print(f\"CBOW cosine similarity between {word1} and {word2}: \", similarity_cbow)\n",
    "else:\n",
    "    print(f\"One or both words are not in vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Implementation attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity (data, science): 0.15628131687288663\n",
      "Cosine Similarity (data, really): -0.37585027669960913\n",
      "Cosine Similarity (data, science) is larger than (data, really): TRUE\n",
      "Euclidean Distance (data, science): 4.40646638292033\n",
      "Euclidean Distance (data, really): 5.857802244317426\n",
      "Euclidean Distance (data, science) is smaller than (data, really): TRUE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, corpus, embed_size, window_size, learning_rate, epochs):\n",
    "        self.corpus = corpus\n",
    "        self.embed_size = embed_size\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # Create vocabulary\n",
    "        self.words = set(corpus)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.words)}\n",
    "        self.index2word = {i: word for i, word in enumerate(self.words)}\n",
    "        self.vocab_size = len(self.words)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.input_word = np.random.randn(self.vocab_size, self.embed_size)\n",
    "\n",
    "        self.output_cotext_words = np.random.randn(self.embed_size, self.vocab_size)\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.loss = 0\n",
    "            for word_index, word in enumerate(self.corpus):\n",
    "                # We are going to predict the context words from the current word\n",
    "                # The context words are the words that fall within the window size\n",
    "                # around the current word\n",
    "                start = max(0, word_index - self.window_size)\n",
    "                end = min(word_index + self.window_size, len(self.corpus))\n",
    "                context_indices = [self.word2index[w] for w in self.corpus[start:end] if w != word]\n",
    "                \n",
    "                for context_index in context_indices:\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    input_word_embedding = self.input_word[context_index]\n",
    "          \n",
    "                    # print(self.embedding_to_word(input_word_embedding))\n",
    "                    dot_product = np.dot(input_word_embedding, self.output_cotext_words)\n",
    "\n",
    "                    softmaxed_probability = self.softmax(dot_product)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    self.loss += -np.log(softmaxed_probability[self.word2index[word]])\n",
    "                    \n",
    "                    # Backpropagation\n",
    "                    e = softmaxed_probability.copy()\n",
    "                    # Subtract 1 from the word that was actually the context word\n",
    "                    e[self.word2index[word]] -= 1\n",
    "                    \n",
    "                    #This is the gradient that shows how much the output embedding should change to minimize the loss\n",
    "                    gradient_for_output_embedding = np.outer(input_word_embedding, e)\n",
    "                    #This is the gradient that shows how much the input embedding should change to minimize the loss\n",
    "                    gradient_for_input_embedding = np.dot(self.output_cotext_words, e.T)\n",
    "                    \n",
    "                    # Update weights\n",
    "                    #Updating output embedding to take a STEP TOWARDS OPPOSITE TO THE GRADIENT\n",
    "                    self.output_cotext_words -= self.learning_rate * gradient_for_output_embedding\n",
    "\n",
    "                    #Updating input embedding ONLY FOR THE CONTEXT WORD in the context matrix\n",
    "                    self.input_word[context_index] -= self.learning_rate * gradient_for_input_embedding\n",
    "            \n",
    "            # print(f'Epoch: {epoch + 1}/{self.epochs}, Loss: {self.loss/len(self.corpus)}')\n",
    "            \n",
    "   \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / exp_x.sum(axis=0)\n",
    "    \n",
    "    def get_word_vector(self, word):\n",
    "        return self.input_word[self.word2index[word]]\n",
    "    \n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def euclidean_distance(vec1, vec2):\n",
    "    return np.linalg.norm(vec1 - vec2)\n",
    "\n",
    "\n",
    "\n",
    "#read training_text.txt and split it into words\n",
    "corpus = open(\"training_text.txt\", \"r\").read().split()\n",
    "\n",
    "\n",
    "model = Word2Vec(corpus, embed_size=19, window_size=2, learning_rate=0.01, epochs=1000)\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Cosine Similarity (data, science):\", cosine_similarity(model.get_word_vector(\"data\"), model.get_word_vector(\"science\")))\n",
    "print(\"Cosine Similarity (data, really):\", cosine_similarity(model.get_word_vector(\"data\"), model.get_word_vector(\"really\")))\n",
    "\n",
    "#compare above two cosine similarities with each other and print the smaller pair\n",
    "if cosine_similarity(model.get_word_vector(\"data\"), model.get_word_vector(\"science\")) > cosine_similarity(model.get_word_vector(\"data\"), model.get_word_vector(\"really\")):\n",
    "    print(\"Cosine Similarity (data, science) is larger than (data, really): TRUE\")\n",
    "else:\n",
    "    print(\"Cosine Similarity (data, science) is smaller than (data, really): FALSE\")\n",
    "\n",
    "\n",
    "print(\"Euclidean Distance (data, science):\", euclidean_distance(model.get_word_vector(\"data\"), model.get_word_vector(\"science\")))\n",
    "print(\"Euclidean Distance (data, really):\", euclidean_distance(model.get_word_vector(\"data\"), model.get_word_vector(\"really\")))\n",
    "\n",
    "#compare above two euclidean distances with each other and print the smaller pair\n",
    "if euclidean_distance(model.get_word_vector(\"data\"), model.get_word_vector(\"science\")) > euclidean_distance(model.get_word_vector(\"data\"), model.get_word_vector(\"really\")):\n",
    "    print(\"Euclidean Distance (data, science) is larger than (data, really): FALSE\")\n",
    "else:\n",
    "    print(\"Euclidean Distance (data, science) is smaller than (data, really): TRUE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
