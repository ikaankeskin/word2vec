{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "df_read = pd.read_parquet('CL_fr-en (1).parquet', engine='pyarrow')\n",
    "\n",
    "df_read = df_read.dropna()\n",
    "\n",
    "df_read['fr'] = df_read['fr'].str.lower()\n",
    "df_read['en'] = df_read['en'].str.lower()\n",
    "\n",
    "df_read['fr'] = df_read['fr'].str.replace('[^\\w\\s]','')\n",
    "df_read['en'] = df_read['en'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "\n",
    "#remove punctuation\n",
    "import string\n",
    "df_read['fr'] = df_read['fr'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "df_read['en'] = df_read['en'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "df_read=df_read.head(1000)\n",
    "concatdf = pd.concat([df_read['en'], df_read['fr']], ignore_index=True)\n",
    "#save concatdf to a txt file without quotes\n",
    "concatdf.to_csv('concatdf.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "class SentencePieceTokenizer:\n",
    "    def __init__(self, input_file, model_prefix='sentence_piece', vocab_size=1000, model_type='unigram', character_coverage=1.0):\n",
    "        self.input_file = input_file\n",
    "        self.model_prefix = model_prefix\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_type = model_type\n",
    "        self.character_coverage = character_coverage\n",
    "        self.tokenized_corpus = []\n",
    "\n",
    "    def train_tokenizer(self):\n",
    "        spm.SentencePieceTrainer.train(f'--input={self.input_file} '\n",
    "                                       f'--model_prefix={self.model_prefix} '\n",
    "                                       f'--vocab_size={self.vocab_size} '\n",
    "                                       f'--model_type={self.model_type} '\n",
    "                                       f'--character_coverage={self.character_coverage}')\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(f'{self.model_prefix}.model')\n",
    "\n",
    "    def tokenize_corpus(self):\n",
    "        with open(self.input_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                self.tokenized_corpus.append(self.sp.encode_as_pieces(line.strip()))\n",
    "\n",
    "    def get_tokenized_corpus(self):\n",
    "        return self.tokenized_corpus\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return {self.sp.id_to_piece(id): id for id in range(self.sp.get_piece_size())}\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.sp.get_piece_size()\n",
    "\n",
    "\n",
    "class GensimWord2Vec:\n",
    "    def __init__(self, corpus, embed_size, window_size, learning_rate, epochs):\n",
    "        self.corpus = [corpus]\n",
    "        self.embed_size = embed_size\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self):\n",
    "        self.model = Word2Vec(sentences=self.corpus,\n",
    "                              vector_size=self.embed_size,\n",
    "                              window=self.window_size,\n",
    "                              alpha=self.learning_rate,\n",
    "                              epochs=self.epochs,\n",
    "                              sg=1,  # Use skip-gram\n",
    "                              workers=1)  # Single worker for reproducibility\n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        return self.model.wv[word]\n",
    "    \n",
    "    def find_closest_word(self, word):\n",
    "        max_similarity = -1.0  # Start with the lowest possible similarity value\n",
    "        closest_word = None\n",
    "\n",
    "        if word not in self.model.wv:  # Check if the word exists in the model\n",
    "            return f\"{word} not found in model\"\n",
    "\n",
    "        word_vector = np.array(self.model.wv[word]).reshape(1, -1)\n",
    "        for w in self.model.wv.index_to_key:  # Loop through the model's vocab\n",
    "            if w == word:  # Skip the word itself\n",
    "                continue\n",
    "            w_vector = np.array(self.model.wv[w]).reshape(1, -1)\n",
    "            similarity_output = cosine_similarity(word_vector, w_vector)\n",
    "\n",
    "            # Check the type of the output\n",
    "            if np.isscalar(similarity_output):\n",
    "                similarity = similarity_output\n",
    "            else:\n",
    "                similarity = similarity_output[0][0]\n",
    "\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                closest_word = w\n",
    "\n",
    "        return closest_word\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, target, context):\n",
    "        in_vectors = self.in_embed(target)\n",
    "        out_vectors = self.out_embed(context)\n",
    "        scores = torch.matmul(in_vectors, out_vectors.t())\n",
    "        return scores\n",
    "\n",
    "    def train_model(self, corpus, vocab, window_size, learning_rate, epochs, negative_samples=5):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for idx, word in enumerate(corpus):\n",
    "                # Get the context words based on window size\n",
    "                start = max(0, idx - window_size)\n",
    "                end = min(len(corpus), idx + window_size + 1)\n",
    "                context_words = corpus[start:idx] + corpus[idx+1:end]\n",
    "\n",
    "                for context_word in context_words:\n",
    "                    target_tensor = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "                    context_tensor = torch.tensor([vocab[context_word]], dtype=torch.long)\n",
    "                    \n",
    "                    negative_context = torch.multinomial(torch.tensor(list(vocab.values()), dtype=torch.float), negative_samples, replacement=True)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    scores = self(target_tensor, torch.cat([context_tensor, negative_context]))\n",
    "                    \n",
    "                    # Create labels: 1 for the correct context word and 0 for the negative samples\n",
    "                    labels = torch.cat([torch.tensor([1.0]), torch.zeros(negative_samples)]).unsqueeze(0)\n",
    "                    \n",
    "                    loss = loss_fn(scores, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    \n",
    "    def get_word_vector(self, word, vocab):\n",
    "        word_idx = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "        return self.in_embed(word_idx).squeeze().detach().numpy()\n",
    "    \n",
    "    def find_closest_word(self, word, vocab):\n",
    "            max_similarity = -1.0  # Start with the lowest possible similarity value\n",
    "            closest_word = None\n",
    "\n",
    "            if word not in vocab:  # Check if the word exists in the vocab\n",
    "                return f\"{word} not found in vocab\"\n",
    "\n",
    "            word_idx = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "            word_vector = self.in_embed(word_idx).detach().numpy().reshape(1, -1)\n",
    "\n",
    "            for w, idx in vocab.items():  # Loop through the vocab\n",
    "                if w == word:  # Skip the word itself\n",
    "                    continue\n",
    "                w_idx = torch.tensor([idx], dtype=torch.long)\n",
    "                w_vector = self.in_embed(w_idx).detach().numpy().reshape(1, -1)\n",
    "\n",
    "                similarity_output = cosine_similarity(word_vector, w_vector)\n",
    "                \n",
    "                # Check the type of the output\n",
    "                if np.isscalar(similarity_output):\n",
    "                    similarity = similarity_output\n",
    "                else:\n",
    "                    similarity = similarity_output[0][0]\n",
    "                \n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    closest_word = w\n",
    "\n",
    "            return closest_word\n",
    "    \n",
    "    \n",
    "\n",
    "def SP_build_vocabulary(vocab):\n",
    "    # Assuming vocab is a dictionary, invert it\n",
    "    return {v: k for k, v in vocab.items()}\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = vec1.squeeze()\n",
    "    vec2 = vec2.squeeze()\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'e'\n",
    "word2 = 'e'\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "# example_word = 36\n",
    "embed_size = 20\n",
    "window_size = 2\n",
    "learning_rate = 0.025\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Piece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 90\n",
      "[['▁', 'T', 'od', 'a', 'y', '▁w', 'e', '▁wi', 'll', '▁be', '▁learning', '▁a', 'b', 'ou', 't', '▁th', 'e', '▁f', 'u', 'nd', 'ame', 'n', 'tal', 's', '▁', 'o', 'f', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', '.', '▁', 'D', 'ata', '▁', 'S', 'ci', 'ence', '▁a', 'nd', '▁statistics', '▁a', 're', '▁h', 'o', 't', '▁a', 'nd', '▁g', 'r', 'o', 'wi', 'ng', '▁', 'fi', 'el', 'd', 's', '▁wi', 'th', '▁', 'al', 'ter', 'n', 'ati', 've', '▁', 'n', 'ame', 's', '▁', 'o', 'f', '▁m', 'a', 'c', 'h', 'ine', '▁learning', ',', '▁ar', 'ti', 'fi', 'ci', 'al', '▁', 'in', 'te', 'll', 'ig', 'ence', ',', '▁b', 'ig', '▁data', ',', '▁', 'e', 't', 'c', '.', '▁I', \"'\", 'm', '▁', 're', 'a', 'll', 'y', '▁ex', 'ci', 'te', 'd', '▁t', 'o', '▁', 'tal', 'k', '▁t', 'o', '▁', 'y', 'ou', '▁a', 'b', 'ou', 't', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', '▁beca', 'use', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', '▁h', 'a', 've', '▁l', 'o', 'ng', '▁be', 'en', '▁a', '▁', 'p', 'a', 's', 's', 'i', 'on', 's', '▁', 'o', 'f', '▁m', 'ine', '.', '▁I', '▁', 'd', 'i', 'd', 'n', \"'\", 't', '▁', 'use', 'd', '▁t', 'o', '▁be', '▁', 've', 'r', 'y', '▁go', 'od', '▁a', 't', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', '▁b', 'u', 't', '▁a', 'f', 'ter', '▁s', 't', 'u', 'd', 'y', 'ing', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', '▁f', 'o', 'r', '▁a', '▁l', 'o', 'ng', '▁', 'ti', 'me', ',', '▁I', '▁go', 't', '▁be', 't', 'ter', '▁a', 'nd', '▁be', 't', 'ter', '▁a', 't', '▁', 'it', '▁', 'un', 'ti', 'l', '▁I', '▁beca', 'me', '▁a', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', '▁ex', 'p', 'er', 't', '.', '▁I', \"'\", 'm', '▁', 're', 'a', 'll', 'y', '▁ex', 'ci', 'te', 'd', '▁t', 'o', '▁', 'tal', 'k', '▁t', 'o', '▁', 'y', 'ou', '▁a', 'b', 'ou', 't', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', ',', '▁th', 'an', 'k', 's', '▁f', 'o', 'r', '▁l', 'ist', 'en', 'ing', '▁t', 'o', '▁', 'me', '▁', 'tal', 'k', '▁a', 'b', 'ou', 't', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', '.']]\n",
      "<class 'list'>\n",
      "{0: '<unk>', 1: '<s>', 2: '</s>', 3: '▁', 4: '▁a', 5: 'o', 6: 't', 7: 'ci', 8: 'nd', 9: 'ence', 10: '▁statistics', 11: '▁data', 12: '▁s', 13: 's', 14: 'd', 15: 'y', 16: 'ou', 17: '▁t', 18: 'a', 19: ',', 20: '.', 21: '▁I', 22: '▁be', 23: 'n', 24: 'r', 25: 'f', 26: 'b', 27: 'k', 28: 'ter', 29: 'e', 30: 'll', 31: 'tal', 32: 'i', 33: '▁f', 34: 'ng', 35: 'u', 36: \"'\", 37: 've', 38: '▁ex', 39: 'me', 40: 'al', 41: 'te', 42: 're', 43: '▁l', 44: 'ti', 45: 'l', 46: 'm', 47: 'c', 48: 'p', 49: '▁learning', 50: 'use', 51: 'ig', 52: '▁beca', 53: 'ine', 54: '▁go', 55: '▁wi', 56: '▁m', 57: 'ame', 58: '▁th', 59: '▁b', 60: 'fi', 61: 'ing', 62: 'en', 63: '▁h', 64: 'h', 65: 'un', 66: 'od', 67: 'wi', 68: 'th', 69: '▁w', 70: 'D', 71: 'S', 72: 'T', 73: '▁g', 74: 'ata', 75: 'er', 76: 'ist', 77: 'in', 78: 'an', 79: '▁ar', 80: 'ati', 81: 'on', 82: 'it', 83: 'el', 84: '▁st', 85: 'x', 86: 'v', 87: 'w', 88: 'I', 89: 'g'}\n",
      "<class 'dict'>\n",
      "90\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# SentencePieceTokenizer\n",
    "tokenizer = SentencePieceTokenizer('training_text.txt', model_prefix='sentence_piece', vocab_size=90)\n",
    "tokenizer.train_tokenizer()\n",
    "\n",
    "# Tokenize the entire corpus\n",
    "tokenizer.tokenize_corpus()\n",
    "\n",
    "# Get tokenized corpus, vocab, and vocab_size\n",
    "corpus = tokenizer.get_tokenized_corpus()\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab = SP_build_vocabulary(vocab)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "for sentence in corpus:\n",
    "    for word in sentence:\n",
    "        if not isinstance(word, str):\n",
    "            print(f\"Non-string element found: {word}\")\n",
    "\n",
    "print(corpus)\n",
    "print(type(corpus))\n",
    "print(vocab)\n",
    "print(type(vocab))\n",
    "print(vocab_size)\n",
    "print(type(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random word selector\n",
    "import random\n",
    "example_word = random.choice(list(vocab.keys()))\n",
    "print(example_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim Model Training\n",
    "print(\"Training Gensim Model...\")\n",
    "modelGensim = GensimWord2Vec(corpus, embed_size, window_size, learning_rate, epochs)\n",
    "modelGensim.train()\n",
    "\n",
    "# Test the Gensim model (change 'example_word' to any word in your corpus)\n",
    "\n",
    "if example_word in vocab:\n",
    "    print(f\"Gensim Vector for {example_word}: \", modelGensim.get_word_vector(example_word))\n",
    "    vec1_gensim = modelGensim.get_word_vector(word1)\n",
    "    vec2_gensim = modelGensim.get_word_vector(word2)\n",
    "    similarity_gensim = cosine_similarity(vec1_gensim, vec2_gensim)\n",
    "    print(f\"Gensim cosine similarity between {word1} and {word2}: \", similarity_gensim)\n",
    "else:\n",
    "    print(f\"{example_word} not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkipGram Model Training\n",
    "print(\"\\nTraining SkipGram Model...\")\n",
    "modelSkipGram = SkipGram(vocab_size, embed_size)\n",
    "modelSkipGram.train_model(corpus, vocab, window_size, learning_rate, epochs)\n",
    "\n",
    "# Test the SkipGram model (change 'example_word' to any word in your corpus)\n",
    "if example_word in vocab:\n",
    "    print(f\"SkipGram Vector for {example_word}: \", modelSkipGram.get_word_vector(example_word, vocab))\n",
    "    vec1_skipgram = modelSkipGram.get_word_vector(word1, vocab)\n",
    "    vec2_skipgram = modelSkipGram.get_word_vector(word2, vocab)\n",
    "    similarity_skipgram = cosine_similarity(vec1_skipgram, vec2_skipgram)\n",
    "    print(f\"SkipGram cosine similarity between {word1} and {word2}: \", similarity_skipgram)\n",
    "else:\n",
    "    print(f\"{example_word} not in vocabulary.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest word to 'data' using Gensim: statistics\n",
      "Closest word to 'data' using SkipGram: thanks\n"
     ]
    }
   ],
   "source": [
    "closest_gensim = modelGensim.find_closest_word('data')\n",
    "print(f\"Closest word to 'data' using Gensim: {closest_gensim}\")\n",
    "\n",
    "closest_skipgram = modelSkipGram.find_closest_word('data', vocab)\n",
    "print(f\"Closest word to 'data' using SkipGram: {closest_skipgram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim cosine similarity between data and science:  0.9797871\n",
      "SkipGram cosine similarity between data and science:  0.25304967\n",
      "CBOW cosine similarity between data and science:  -0.015298413\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "word1 = 'data'\n",
    "word2 = 'science'\n",
    "\n",
    "if word1 in vocab and word2 in vocab:\n",
    "    # Gensim Model\n",
    "    vec1_gensim = modelGensim.get_word_vector(word1)\n",
    "    vec2_gensim = modelGensim.get_word_vector(word2)\n",
    "    similarity_gensim = cosine_similarity(vec1_gensim, vec2_gensim)\n",
    "    print(f\"Gensim cosine similarity between {word1} and {word2}: \", similarity_gensim)\n",
    "\n",
    "    # SkipGram Model\n",
    "    vec1_skipgram = modelSkipGram.get_word_vector(word1, vocab)\n",
    "    vec2_skipgram = modelSkipGram.get_word_vector(word2, vocab)\n",
    "    similarity_skipgram = cosine_similarity(vec1_skipgram, vec2_skipgram)\n",
    "    print(f\"SkipGram cosine similarity between {word1} and {word2}: \", similarity_skipgram)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
