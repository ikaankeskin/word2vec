{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, corpus, embed_size, window_size, learning_rate, epochs):\n",
    "        self.corpus = corpus\n",
    "        self.embed_size = embed_size\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # Create vocabulary\n",
    "        self.words = set(corpus)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.words)}\n",
    "        self.index2word = {i: word for i, word in enumerate(self.words)}\n",
    "        self.vocab_size = len(self.words)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.input_word = np.random.randn(self.vocab_size, self.embed_size)\n",
    "\n",
    "        self.output_cotext_words = np.random.randn(self.embed_size, self.vocab_size)\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.loss = 0\n",
    "            for word_index, word in enumerate(self.corpus):\n",
    "                # We are going to predict the context words from the current word\n",
    "                # The context words are the words that fall within the window size\n",
    "                # around the current word\n",
    "                start = max(0, word_index - self.window_size)\n",
    "                end = min(word_index + self.window_size, len(self.corpus))\n",
    "                context_indices = [self.word2index[w] for w in self.corpus[start:end] if w != word]\n",
    "                \n",
    "                for context_index in context_indices:\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    input_word_embedding = self.input_word[context_index]\n",
    "          \n",
    "                    # print(self.embedding_to_word(input_word_embedding))\n",
    "                    dot_product = np.dot(input_word_embedding, self.output_cotext_words)\n",
    "\n",
    "                    softmaxed_probability = self.softmax(dot_product)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    self.loss += -np.log(softmaxed_probability[self.word2index[word]])\n",
    "                    \n",
    "                    # Backpropagation\n",
    "                    e = softmaxed_probability.copy()\n",
    "                    # Subtract 1 from the word that was actually the context word\n",
    "                    e[self.word2index[word]] -= 1\n",
    "                    \n",
    "                    #This is the gradient that shows how much the output embedding should change to minimize the loss\n",
    "                    gradient_for_output_embedding = np.outer(input_word_embedding, e)\n",
    "                    #This is the gradient that shows how much the input embedding should change to minimize the loss\n",
    "                    gradient_for_input_embedding = np.dot(self.output_cotext_words, e.T)\n",
    "                    \n",
    "                    # Update weights\n",
    "                    #Updating output embedding to take a STEP TOWARDS OPPOSITE TO THE GRADIENT\n",
    "                    self.output_cotext_words -= self.learning_rate * gradient_for_output_embedding\n",
    "\n",
    "                    #Updating input embedding ONLY FOR THE CONTEXT WORD in the context matrix\n",
    "                    self.input_word[context_index] -= self.learning_rate * gradient_for_input_embedding\n",
    "            \n",
    "            # print(f'Epoch: {epoch + 1}/{self.epochs}, Loss: {self.loss/len(self.corpus)}')\n",
    "            \n",
    "   \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / exp_x.sum(axis=0)\n",
    "    \n",
    "    def get_word_vector(self, word):\n",
    "        return self.input_word[self.word2index[word]]\n",
    "    \n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def euclidean_distance(vec1, vec2):\n",
    "    return np.linalg.norm(vec1 - vec2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Corpus Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity (quick, brown): 0.10410215449008678\n",
      "Euclidean Distance (quick, brown): 5.608967870998298\n",
      "Cosine Similarity (quick, brown) is larger than (quick, dog): TRUE\n",
      "Cosine Similarity (quick, dog): -0.22225721139339435\n",
      "Euclidean Distance (quick, dog): 6.449068007238908\n",
      "Euclidean Distance (quick, brown) is smaller than (quick, dog): TRUE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = \"the quick brown fox jumped over the lazy dog\".split()\n",
    "\n",
    "\n",
    "model = Word2Vec(corpus, embed_size=20, window_size=2, learning_rate=0.01, epochs=1000)\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate for \"quick\" and \"brown\"\n",
    "print(\"Cosine Similarity (quick, brown):\", cosine_similarity(model.get_word_vector(\"quick\"), model.get_word_vector(\"brown\")))\n",
    "print(\"Euclidean Distance (quick, brown):\", euclidean_distance(model.get_word_vector(\"quick\"), model.get_word_vector(\"brown\")))\n",
    "#compare above two cosine similarities with each other and print the smaller pair\n",
    "if cosine_similarity(model.get_word_vector(\"quick\"), model.get_word_vector(\"brown\")) > cosine_similarity(model.get_word_vector(\"quick\"), model.get_word_vector(\"dog\")):\n",
    "    print(\"Cosine Similarity (quick, brown) is larger than (quick, dog): TRUE\")\n",
    "else:\n",
    "    print(\"Cosine Similarity (quick, brown) is smaller than (quick, dog): FALSE\")\n",
    "\n",
    "# Calculate for \"quick\" and \"dog\"\n",
    "print(\"Cosine Similarity (quick, dog):\", cosine_similarity(model.get_word_vector(\"quick\"), model.get_word_vector(\"dog\")))\n",
    "print(\"Euclidean Distance (quick, dog):\", euclidean_distance(model.get_word_vector(\"quick\"), model.get_word_vector(\"dog\")))\n",
    "#compare above two euclidean distances with each other and print the smaller pair\n",
    "if euclidean_distance(model.get_word_vector(\"quick\"), model.get_word_vector(\"brown\")) > euclidean_distance(model.get_word_vector(\"quick\"), model.get_word_vector(\"dog\")):\n",
    "    print(\"Euclidean Distance (quick, brown) is larger than (quick, dog): FALSE\")\n",
    "else:\n",
    "    print(\"Euclidean Distance (quick, brown) is smaller than (quick, dog): TRUE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slightly bigger Corpus Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity (data, science): 0.15936889830770104\n",
      "Cosine Similarity (data, really): -0.2637223323415979\n",
      "Cosine Similarity (data, science) is larger than (data, really): TRUE\n",
      "Euclidean Distance (data, science): 4.445672355379358\n",
      "Euclidean Distance (data, really): 6.493874619544722\n",
      "Euclidean Distance (data, science) is smaller than (data, really): TRUE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#read training_text.txt and split it into words\n",
    "corpus = open(\"training_text.txt\", \"r\").read().split()\n",
    "\n",
    "\n",
    "model = Word2Vec(corpus, embed_size=19, window_size=2, learning_rate=0.01, epochs=1000)\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Cosine Similarity (data, science):\", cosine_similarity(model.get_word_vector(\"data\"), model.get_word_vector(\"science\")))\n",
    "print(\"Cosine Similarity (data, really):\", cosine_similarity(model.get_word_vector(\"data\"), model.get_word_vector(\"really\")))\n",
    "\n",
    "#compare above two cosine similarities with each other and print the smaller pair\n",
    "if cosine_similarity(model.get_word_vector(\"data\"), model.get_word_vector(\"science\")) > cosine_similarity(model.get_word_vector(\"data\"), model.get_word_vector(\"really\")):\n",
    "    print(\"Cosine Similarity (data, science) is larger than (data, really): TRUE\")\n",
    "else:\n",
    "    print(\"Cosine Similarity (data, science) is smaller than (data, really): FALSE\")\n",
    "\n",
    "\n",
    "print(\"Euclidean Distance (data, science):\", euclidean_distance(model.get_word_vector(\"data\"), model.get_word_vector(\"science\")))\n",
    "print(\"Euclidean Distance (data, really):\", euclidean_distance(model.get_word_vector(\"data\"), model.get_word_vector(\"really\")))\n",
    "\n",
    "#compare above two euclidean distances with each other and print the smaller pair\n",
    "if euclidean_distance(model.get_word_vector(\"data\"), model.get_word_vector(\"science\")) > euclidean_distance(model.get_word_vector(\"data\"), model.get_word_vector(\"really\")):\n",
    "    print(\"Euclidean Distance (data, science) is larger than (data, really): FALSE\")\n",
    "else:\n",
    "    print(\"Euclidean Distance (data, science) is smaller than (data, really): TRUE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
