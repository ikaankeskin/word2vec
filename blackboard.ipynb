{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today we will be learning about the fundamentals of data science and statistics. Data Science and statistics are hot and growing fields with alternative names of machine learning, artificial intelligence, big data, etc. I'm really excited to talk to you about data science and statistics because data science and statistics have long been a passions of mine. I didn't used to be very good at data science and statistics but after studying data science and statistics for a long time, I got better and better at it until I became a data science and statistics expert. I'm really excited to talk to you about data science and statistics, thanks for listening to me talk about data science and statistics.\n",
      "['today', 'learning', 'fundamentals', 'data', 'science', 'statistics', 'data', 'science', 'statistics', 'hot', 'growing', 'fields', 'alternative', 'names', 'machine', 'learning', 'artificial', 'intelligence', 'big', 'data', 'etc', 'im', 'really', 'excited', 'talk', 'data', 'science', 'statistics', 'data', 'science', 'statistics', 'long', 'passions', 'mine', 'didnt', 'used', 'good', 'data', 'science', 'statistics', 'studying', 'data', 'science', 'statistics', 'long', 'time', 'got', 'better', 'better', 'became', 'data', 'science', 'statistics', 'expert', 'im', 'really', 'excited', 'talk', 'data', 'science', 'statistics', 'thanks', 'listening', 'talk', 'data', 'science', 'statistics']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "with open('stopwords.txt') as f:\n",
    "    stopwords = f.read().replace('\\n',' ').split()\n",
    "\n",
    "with open('training_text.txt', encoding='utf-8') as f:\n",
    "    text = f.read().replace('\\n','')\n",
    "    print(text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ''.join([t for t in text if t not in list('0123456789')])\n",
    "    text = text.replace('”', '').replace('“', '').replace('’', '').lower().split()\n",
    "\n",
    "corpus = [w for w in text if w not in stopwords][:2000]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# SkipGram with Pytorch\n",
    "\n",
    "Input Embedding Layer: This layer converts a target word to its embedding representation. It's simply a lookup in the embedding matrix.\n",
    "\n",
    "Dot Product: The embedding of the target word is then multiplied (dot product) with all other word embeddings in the output embedding layer. This essentially scores how well the target word's embedding matches with all possible context words.\n",
    "\n",
    "Softmax Layer: The output of the dot product operation undergoes a softmax operation to turn the scores into probabilities. This is done outside the model in your current code implementation, where the CrossEntropyLoss function essentially combines the softmax and negative log likelihood loss.\n",
    "\n",
    "Output Embedding Layer: This layer contains embeddings for context words. We take the dot product of the target word's embedding (from the input layer) with every embedding in the output layer to produce a score for each potential context word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/1000, Loss: 7.142135645916213\n",
      "Epoch: 200/1000, Loss: 7.054139295620705\n",
      "Epoch: 300/1000, Loss: 7.021365857836026\n",
      "Epoch: 400/1000, Loss: 6.999107832339273\n",
      "Epoch: 500/1000, Loss: 6.981961687998985\n",
      "Epoch: 600/1000, Loss: 6.968122459169644\n",
      "Epoch: 700/1000, Loss: 6.956634880891487\n",
      "Epoch: 800/1000, Loss: 6.946903927112693\n",
      "Epoch: 900/1000, Loss: 6.938531344506278\n",
      "Epoch: 1000/1000, Loss: 6.931239293582403\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, corpus, embed_size=18, window_size=2, learning_rate=0.01, epochs=1000):\n",
    "        super(SkipGram, self).__init__()\n",
    "\n",
    "        self.corpus = corpus\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.words = set(self.corpus)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.words)}\n",
    "        self.index2word = {i: word for i, word in enumerate(self.words)}\n",
    "        self.vocab_size = len(self.words)\n",
    "\n",
    "        self.in_embed = nn.Embedding(self.vocab_size, embed_size)\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, embed_size)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def forward(self, target):\n",
    "        in_vector = self.in_embed(target)\n",
    "        out_vectors = self.out_embed.weight\n",
    "        scores = torch.matmul(in_vector, out_vectors.T)\n",
    "        return scores\n",
    "\n",
    "    def get_context_indices(self, word_index):\n",
    "        start = max(0, word_index - self.window_size)\n",
    "        end = min(word_index + self.window_size + 1, len(self.corpus))\n",
    "        return list(range(start, word_index)) + list(range(word_index + 1, end))\n",
    "\n",
    "    def train_model(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            for word_index, word in enumerate(self.corpus):\n",
    "                context_indices = self.get_context_indices(word_index)\n",
    "                for context_index in context_indices:\n",
    "                    input_word = torch.tensor([self.word2index[word]], dtype=torch.long)\n",
    "                    true_context = torch.tensor([self.word2index[self.corpus[context_index]]], dtype=torch.long)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    log_probs = self(input_word)\n",
    "                    loss = self.criterion(log_probs, true_context)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f'Epoch: {epoch + 1}/{self.epochs}, Loss: {total_loss / len(self.corpus)}')\n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        word_index = torch.tensor([self.word2index[word]], dtype=torch.long)\n",
    "        return self.in_embed(word_index).squeeze().detach()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def euclidean_distance(vec1, vec2):\n",
    "    return np.linalg.norm(vec1 - vec2)\n",
    "\n",
    "\n",
    "modelSkipGram = SkipGram(corpus)\n",
    "modelSkipGram.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/1000, Loss: 0.12018049714898457\n",
      "Epoch: 200/1000, Loss: 0.10859312155716636\n",
      "Epoch: 300/1000, Loss: 0.1027249304663961\n",
      "Epoch: 400/1000, Loss: 0.09853904671049281\n",
      "Epoch: 500/1000, Loss: 0.09524742339948064\n",
      "Epoch: 600/1000, Loss: 0.09254507572483424\n",
      "Epoch: 700/1000, Loss: 0.09027053312934526\n",
      "Epoch: 800/1000, Loss: 0.08832323910523407\n",
      "Epoch: 900/1000, Loss: 0.08663467905054532\n",
      "Epoch: 1000/1000, Loss: 0.0851551952180861\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, corpus, embed_size=19, window_size=2, learning_rate=0.01, epochs=1000):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        self.corpus = corpus\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.words = set(self.corpus)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.words)}\n",
    "        self.index2word = {i: word for i, word in enumerate(self.words)}\n",
    "        self.vocab_size = len(self.words)\n",
    "\n",
    "        self.in_embed = nn.Embedding(self.vocab_size, embed_size)\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, embed_size)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def forward(self, context):\n",
    "        context_vectors = self.in_embed(context)\n",
    "        embed_sum = torch.sum(context_vectors, dim=1)\n",
    "        scores = torch.matmul(embed_sum, self.out_embed.weight.T)\n",
    "        return scores\n",
    "\n",
    "    def get_context_indices(self, word_index):\n",
    "        start = max(0, word_index - self.window_size)\n",
    "        end = min(word_index + self.window_size + 1, len(self.corpus))\n",
    "        return list(range(start, word_index)) + list(range(word_index + 1, end))\n",
    "\n",
    "    def train_model(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            for word_index, word in enumerate(self.corpus):\n",
    "                context_indices = self.get_context_indices(word_index)\n",
    "                context_words = torch.tensor([self.word2index[self.corpus[idx]] for idx in context_indices], dtype=torch.long)\n",
    "                target = torch.tensor([self.word2index[word]], dtype=torch.long)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                log_probs = self(context_words.unsqueeze(0))\n",
    "                loss = self.criterion(log_probs, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f'Epoch: {epoch + 1}/{self.epochs}, Loss: {total_loss / len(self.corpus)}')\n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        word_index = torch.tensor([self.word2index[word]], dtype=torch.long)\n",
    "        return self.in_embed(word_index).squeeze().detach().numpy()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def euclidean_distance(vec1, vec2):\n",
    "    return np.linalg.norm(vec1 - vec2)\n",
    "\n",
    "modelCBOW = CBOW(corpus)\n",
    "modelCBOW.train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'learning', 'fundamentals', 'data', 'science', 'statistics', 'data', 'science', 'statistics', 'hot', 'growing', 'fields', 'alternative', 'names', 'machine', 'learning', 'artificial', 'intelligence', 'big', 'data', 'etc', 'im', 'really', 'excited', 'talk', 'data', 'science', 'statistics', 'data', 'science', 'statistics', 'long', 'passions', 'mine', 'didnt', 'used', 'good', 'data', 'science', 'statistics', 'studying', 'data', 'science', 'statistics', 'long', 'time', 'got', 'better', 'better', 'became', 'data', 'science', 'statistics', 'expert', 'im', 'really', 'excited', 'talk', 'data', 'science', 'statistics', 'thanks', 'listening', 'talk', 'data', 'science', 'statistics']\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, stopwords_file, training_text_file):\n",
    "        self.stopwords = self._load_stopwords(stopwords_file)\n",
    "        self.corpus = self._process_text(training_text_file)\n",
    "    \n",
    "    def _load_stopwords(self, stopwords_file):\n",
    "        with open(stopwords_file) as f:\n",
    "            stopwords = f.read().replace('\\n', ' ').split()\n",
    "        return stopwords\n",
    "\n",
    "    def _process_text(self, training_text_file):\n",
    "        with open(training_text_file, encoding='utf-8') as f:\n",
    "            text = f.read().replace('\\n', '')\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            text = ''.join([t for t in text if t not in list('0123456789')])\n",
    "            text = text.replace('”', '').replace('“', '').replace('’', '').lower().split()\n",
    "        return [w for w in text if w not in self.stopwords][:2000]\n",
    "\n",
    "    def get_corpus(self):\n",
    "        return self.corpus\n",
    "\n",
    "\n",
    "class GensimWord2Vec:\n",
    "    def __init__(self, corpus, embed_size, window_size, learning_rate, epochs):\n",
    "        self.corpus = [corpus]\n",
    "        self.embed_size = embed_size\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self):\n",
    "        self.model = Word2Vec(sentences=self.corpus,\n",
    "                              vector_size=self.embed_size,\n",
    "                              window=self.window_size,\n",
    "                              alpha=self.learning_rate,\n",
    "                              epochs=self.epochs,\n",
    "                              sg=1,  # Use skip-gram\n",
    "                              workers=1)  # Single worker for reproducibility\n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        return self.model.wv[word]\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.out_linear = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, target):\n",
    "        in_vectors = self.in_embed(target)\n",
    "        out_logits = self.out_linear(in_vectors)\n",
    "        return out_logits\n",
    "    \n",
    "    def train_model(self, corpus, vocab, learning_rate, epochs):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for idx, word in enumerate(corpus[:-1]):  # -1 to avoid out of bounds\n",
    "                target = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "                context_idx = vocab[corpus[idx + 1]]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                predictions = self(target)\n",
    "                \n",
    "                loss = loss_fn(predictions, torch.tensor([context_idx]))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    def get_word_vector(self, word, vocab):\n",
    "        word_idx = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "        return self.in_embed(word_idx).detach().numpy()\n",
    "    \n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, context, window_size):\n",
    "        # Get the embeddings of the context words\n",
    "        in_vectors = self.in_embed(context)\n",
    "        \n",
    "        # Average the embeddings of the context words\n",
    "        in_vectors_avg = torch.mean(in_vectors, dim=0, keepdim=True)\n",
    "        \n",
    "        # Pass the averaged embeddings through the linear layer\n",
    "        out_logits = self.out_linear(in_vectors_avg)\n",
    "        return out_logits\n",
    "\n",
    "    def train_model(self, corpus, vocab, learning_rate, epochs, window_size=2):\n",
    "        self.out_linear = nn.Linear(self.embed_size, len(vocab)) \n",
    "        \n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for idx, word in enumerate(corpus[window_size:-window_size]):  # Leave space for context words\n",
    "                # Gather context words based on the window size\n",
    "                context_words = corpus[idx-window_size:idx] + corpus[idx+1:idx+1+window_size]\n",
    "                context_indices = [vocab[w] for w in context_words]\n",
    "                \n",
    "                # Convert to tensor\n",
    "                context = torch.tensor(context_indices, dtype=torch.long)\n",
    "                target_idx = vocab[word]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                predictions = self(context, window_size)\n",
    "                \n",
    "                loss = loss_fn(predictions, torch.tensor([target_idx]))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def get_word_vector(self, word, vocab):\n",
    "        word_idx = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "        return self.in_embed(word_idx).detach().numpy()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Vocabulary builder\n",
    "def build_vocabulary(corpus):\n",
    "    vocab = {}\n",
    "    for word in corpus:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CBOW Model...\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and process the data\n",
    "processor = TextProcessor('stopwords.txt', 'training_text.txt')\n",
    "corpus = processor.get_corpus()\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = build_vocabulary(corpus)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "# CBOW Model\n",
    "print(\"\\nTraining CBOW Model...\")\n",
    "modelCBOW = CBOW(vocab_size, 20)\n",
    "modelCBOW.train_model(corpus, vocab, 0.025, 1000, window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gensim Model...\n",
      "Gensim Vector for data:  [-0.03911165  0.22466028  0.15663825  0.23718707 -0.17009805 -0.32686406\n",
      "  0.04607087  0.62932575 -0.24616416 -0.03631771 -0.08447191  0.0153377\n",
      " -0.01536816  0.06978628 -0.05106831 -0.19504882  0.37443215 -0.2380562\n",
      " -0.35111517 -0.24540682]\n",
      "\n",
      "Training SkipGram Model...\n",
      "SkipGram Vector for data:  [[-0.05340152  1.3186854  -0.99223226  0.8127032   1.1575003   0.49402523\n",
      "   1.6774234   0.19818336  0.08226159 -0.268004   -0.8270626   1.0272753\n",
      "  -0.01924454  0.49821365  0.0542079   0.09707388  0.55477446 -0.53761685\n",
      "  -1.623687   -0.7710329 ]]\n",
      "\n",
      "Training CBOW Model...\n",
      "CBOW Vector for data:  [[-1.7172759  -1.2148739  -0.21443495 -0.59840095  0.21292756  1.4785212\n",
      "  -1.8608963  -2.6497774   0.11688966  0.33669746 -1.4511781   4.2494607\n",
      "   0.22541954  2.0907726   0.53882873  3.0997822  -0.80189586 -2.754468\n",
      "   0.9497787  -0.35829023]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and process the data\n",
    "processor = TextProcessor('stopwords.txt', 'training_text.txt')\n",
    "corpus = processor.get_corpus()\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = build_vocabulary(corpus)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Gensim Model\n",
    "print(\"Training Gensim Model...\")\n",
    "modelGensim = GensimWord2Vec(corpus, 20, 2, 0.025, 1000)\n",
    "modelGensim.train()\n",
    "\n",
    "# Test the Gensim model (change 'example_word' to any word in your corpus)\n",
    "example_word = \"data\"\n",
    "if example_word in vocab:\n",
    "    print(f\"Gensim Vector for {example_word}: \", modelGensim.get_word_vector(example_word))\n",
    "else:\n",
    "    print(f\"{example_word} not in vocabulary.\")\n",
    "\n",
    "# SkipGram Model\n",
    "print(\"\\nTraining SkipGram Model...\")\n",
    "modelSkipGram = SkipGram(vocab_size, 20)\n",
    "modelSkipGram.train_model(corpus, vocab, 0.025, 1000)\n",
    "\n",
    "# Test the SkipGram model (change 'example_word' to any word in your corpus)\n",
    "if example_word in vocab:\n",
    "    print(f\"SkipGram Vector for {example_word}: \", modelSkipGram.get_word_vector(example_word, vocab))\n",
    "else:\n",
    "    print(f\"{example_word} not in vocabulary.\")\n",
    "\n",
    "# CBOW Model\n",
    "print(\"\\nTraining CBOW Model...\")\n",
    "modelCBOW = CBOW(vocab_size, 20)\n",
    "modelCBOW.train_model(corpus, vocab, 0.025, 1000, window_size=2)\n",
    "\n",
    "\n",
    "\n",
    "# Test the CBOW model (change 'example_word' to any word in your corpus)\n",
    "if example_word in vocab:\n",
    "    print(f\"CBOW Vector for {example_word}: \", modelCBOW.get_word_vector(example_word, vocab))\n",
    "else:\n",
    "    print(f\"{example_word} not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim cosine similarity between data and science:  0.9797871\n",
      "SkipGram cosine similarity between data and science:  0.097108334\n",
      "CBOW cosine similarity between data and science:  -0.10008272\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = vec1.squeeze()\n",
    "    vec2 = vec2.squeeze()\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "\n",
    "word1 = 'data'\n",
    "word2 = 'science'\n",
    "\n",
    "if word1 in vocab and word2 in vocab:\n",
    "    # Gensim Model\n",
    "    vec1_gensim = modelGensim.get_word_vector(word1)\n",
    "    vec2_gensim = modelGensim.get_word_vector(word2)\n",
    "    similarity_gensim = cosine_similarity(vec1_gensim, vec2_gensim)\n",
    "    print(f\"Gensim cosine similarity between {word1} and {word2}: \", similarity_gensim)\n",
    "\n",
    "    # SkipGram Model\n",
    "    vec1_skipgram = modelSkipGram.get_word_vector(word1, vocab)\n",
    "    vec2_skipgram = modelSkipGram.get_word_vector(word2, vocab)\n",
    "    similarity_skipgram = cosine_similarity(vec1_skipgram, vec2_skipgram)\n",
    "    print(f\"SkipGram cosine similarity between {word1} and {word2}: \", similarity_skipgram)\n",
    "\n",
    "    # CBOW Model\n",
    "    vec1_cbow = modelCBOW.get_word_vector(word1, vocab)\n",
    "    vec2_cbow = modelCBOW.get_word_vector(word2, vocab)\n",
    "    similarity_cbow = cosine_similarity(vec1_cbow, vec2_cbow)\n",
    "    print(f\"CBOW cosine similarity between {word1} and {word2}: \", similarity_cbow)\n",
    "else:\n",
    "    print(f\"One or both words are not in vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities:\n",
      "Gensim: 0.9797871112823486\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SkipGram.get_word_vector() missing 1 required positional argument: 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ikaan\\AppData\\Local\\KK Code\\word2vec\\gensim.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ikaan/AppData/Local/KK%20Code/word2vec/gensim.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCosine similarities:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ikaan/AppData/Local/KK%20Code/word2vec/gensim.ipynb#X22sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGensim: \u001b[39m\u001b[39m{\u001b[39;00mcosine_similarity_gensim(modelGensim\u001b[39m.\u001b[39mmodel,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mscience\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ikaan/AppData/Local/KK%20Code/word2vec/gensim.ipynb#X22sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSkipGram: \u001b[39m\u001b[39m{\u001b[39;00mcosine_similarity_skipgram(modelSkipGram,\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mscience\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ikaan/AppData/Local/KK%20Code/word2vec/gensim.ipynb#X22sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCBOW: \u001b[39m\u001b[39m{\u001b[39;00mcosine_similarity_cbow(modelCBOW,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mscience\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\ikaan\\AppData\\Local\\KK Code\\word2vec\\gensim.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ikaan/AppData/Local/KK%20Code/word2vec/gensim.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcosine_similarity_skipgram\u001b[39m(model, word1, word2):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ikaan/AppData/Local/KK%20Code/word2vec/gensim.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the cosine similarity between two words from a SkipGram model.\"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ikaan/AppData/Local/KK%20Code/word2vec/gensim.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     vec1 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mget_word_vector(word1)\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ikaan/AppData/Local/KK%20Code/word2vec/gensim.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     vec2 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_word_vector(word2)\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ikaan/AppData/Local/KK%20Code/word2vec/gensim.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cosine_similarity_numpy(vec1, vec2)\n",
      "\u001b[1;31mTypeError\u001b[0m: SkipGram.get_word_vector() missing 1 required positional argument: 'vocab'"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity_numpy(vec1, vec2):\n",
    "    \"\"\"Compute the cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "\n",
    "def cosine_similarity_gensim(model, word1, word2):\n",
    "    \"\"\"Compute the cosine similarity between two words from Gensim's Word2Vec model.\"\"\"\n",
    "    vec1 = model.wv[word1]\n",
    "    vec2 = model.wv[word2]\n",
    "    return cosine_similarity_numpy(vec1, vec2)\n",
    "\n",
    "\n",
    "def cosine_similarity_skipgram(model, word1, word2):\n",
    "    \"\"\"Compute the cosine similarity between two words from a SkipGram model.\"\"\"\n",
    "    vec1 = model.get_word_vector(word1).numpy()\n",
    "    vec2 = model.get_word_vector(word2).numpy()\n",
    "    return cosine_similarity_numpy(vec1, vec2)\n",
    "\n",
    "\n",
    "def cosine_similarity_cbow(model, word1, word2):\n",
    "    \"\"\"Compute the cosine similarity between two words from a CBOW model.\"\"\"\n",
    "    vec1 = model.get_word_vector(word1)\n",
    "    vec2 = model.get_word_vector(word2)\n",
    "    return cosine_similarity_numpy(vec1, vec2)\n",
    "\n",
    "print(\"Cosine similarities:\")\n",
    "print(f\"Gensim: {cosine_similarity_gensim(modelGensim.model, 'data', 'science')}\")\n",
    "print(f\"SkipGram: {cosine_similarity_skipgram(modelSkipGram, 'data', 'science')}\")\n",
    "print(f\"CBOW: {cosine_similarity_cbow(modelCBOW, 'data', 'science')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity (data, science): -0.049212173\n",
      "Cosine Similarity (data, really): -0.13519435\n",
      "Cosine Similarity (data, science) is larger than (data, really): TRUE\n",
      "Euclidean Distance (data, science): 5.355234\n",
      "Euclidean Distance (data, really): 6.0373626\n",
      "Euclidean Distance (data, science) is smaller than (data, really): TRUE\n"
     ]
    }
   ],
   "source": [
    "# word1 = \"data\"\n",
    "# word2 = \"science\"\n",
    "# word3 = \"really\"\n",
    "\n",
    "# print(f\"Cosine Similarity ({word1}, {word2}):\", cosine_similarity(model.get_word_vector(word1), model.get_word_vector(word2)))\n",
    "# print(f\"Cosine Similarity ({word1}, {word3}):\", cosine_similarity(model.get_word_vector(word1), model.get_word_vector(word3)))\n",
    "\n",
    "# #compare above two cosine similarities with each other and print the smaller pair\n",
    "# if cosine_similarity(model.get_word_vector(word1), model.get_word_vector(word2)) > cosine_similarity(model.get_word_vector(word1), model.get_word_vector(word3)):\n",
    "#     print(f\"Cosine Similarity ({word1}, {word2}) is larger than ({word1}, {word3}): TRUE\")\n",
    "# else:\n",
    "#     print(f\"Cosine Similarity ({word1}, {word2}) is smaller than ({word1}, {word3}): FALSE\")\n",
    "\n",
    "# print(f\"Euclidean Distance ({word1}, {word2}):\", euclidean_distance(model.get_word_vector(word1), model.get_word_vector(word2)))\n",
    "# print(f\"Euclidean Distance ({word1}, {word3}):\", euclidean_distance(model.get_word_vector(word1), model.get_word_vector(word3)))\n",
    "\n",
    "\n",
    "# #compare above two euclidean distances with each other and print the smaller pair\n",
    "# if euclidean_distance(model.get_word_vector(word1), model.get_word_vector(word2)) > euclidean_distance(model.get_word_vector(word1), model.get_word_vector(word3)):\n",
    "#     print(f\"Euclidean Distance ({word1}, {word2}) is larger than ({word1}, {word3}): FALSE\")\n",
    "# else:\n",
    "#     print(f\"Euclidean Distance ({word1}, {word2}) is smaller than ({word1}, {word3}): TRUE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
