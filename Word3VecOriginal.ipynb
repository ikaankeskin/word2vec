{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "df_read = pd.read_parquet('CL_fr-en (1).parquet', engine='pyarrow')\n",
    "\n",
    "df_read = df_read.dropna()\n",
    "\n",
    "df_read['fr'] = df_read['fr'].str.lower()\n",
    "df_read['en'] = df_read['en'].str.lower()\n",
    "\n",
    "df_read['fr'] = df_read['fr'].str.replace('[^\\w\\s]','')\n",
    "df_read['en'] = df_read['en'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "\n",
    "#remove punctuation\n",
    "import string\n",
    "df_read['fr'] = df_read['fr'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "df_read['en'] = df_read['en'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "df_read=df_read.head(1000)\n",
    "concatdf = pd.concat([df_read['en'], df_read['fr']], ignore_index=True)\n",
    "#save concatdf to a txt file without quotes\n",
    "concatdf.to_csv('concatdf.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  1000\n",
      "Tokens:  ['▁le', '▁premier', '▁point', '▁nou', 's', '▁dis', 'ons']\n"
     ]
    }
   ],
   "source": [
    "#import sentencepiece tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "#train tokenizer on french and english data on the df_read dataframe\n",
    "spm.SentencePieceTrainer.train('--input=concatdf.txt --model_prefix=concatdf --vocab_size=1000 --model_type=unigram --character_coverage=1.0')\n",
    "\n",
    "#load the trained tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "\n",
    "#load the model\n",
    "sp.load('concatdf.model')\n",
    "\n",
    "#print the vocab size\n",
    "print('Vocab size: ', sp.get_piece_size())\n",
    "\n",
    "#print sample tokens\n",
    "print('Tokens: ', sp.encode_as_pieces('le premier point nous disons'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "class SentencePieceTokenizer:\n",
    "    def __init__(self, input_file, model_prefix='sentence_piece', vocab_size=1000, model_type='unigram', character_coverage=1.0):\n",
    "        self.input_file = input_file\n",
    "        self.model_prefix = model_prefix\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_type = model_type\n",
    "        self.character_coverage = character_coverage\n",
    "        self.tokenized_corpus = []\n",
    "\n",
    "    def train_tokenizer(self):\n",
    "        spm.SentencePieceTrainer.train(f'--input={self.input_file} '\n",
    "                                       f'--model_prefix={self.model_prefix} '\n",
    "                                       f'--vocab_size={self.vocab_size} '\n",
    "                                       f'--model_type={self.model_type} '\n",
    "                                       f'--character_coverage={self.character_coverage}')\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(f'{self.model_prefix}.model')\n",
    "\n",
    "    def tokenize_corpus(self):\n",
    "        with open(self.input_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                self.tokenized_corpus.append(self.sp.encode_as_pieces(line.strip()))\n",
    "\n",
    "    def get_tokenized_corpus(self):\n",
    "        return self.tokenized_corpus\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return {self.sp.id_to_piece(id): id for id in range(self.sp.get_piece_size())}\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.sp.get_piece_size()\n",
    "\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, stopwords_file, training_text_file):\n",
    "        self.stopwords = self._load_stopwords(stopwords_file)\n",
    "        self.corpus = self._process_text(training_text_file)\n",
    "    \n",
    "    def _load_stopwords(self, stopwords_file):\n",
    "        with open(stopwords_file) as f:\n",
    "            stopwords = f.read().replace('\\n', ' ').split()\n",
    "        return stopwords\n",
    "\n",
    "    def _process_text(self, training_text_file):\n",
    "        with open(training_text_file, encoding='utf-8') as f:\n",
    "            text = f.read().replace('\\n', '')\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            text = ''.join([t for t in text if t not in list('0123456789')])\n",
    "            text = text.replace('”', '').replace('“', '').replace('’', '').lower().split()\n",
    "        return [w for w in text if w not in self.stopwords][:2000]\n",
    "\n",
    "    def get_corpus(self):\n",
    "        return self.corpus\n",
    "\n",
    "# processor = TextProcessor('stopwords.txt', 'training_text.txt')\n",
    "\n",
    "\n",
    "# corpus = processor.get_corpus()\n",
    "\n",
    "# # Build the vocabulary\n",
    "# vocab = build_vocabulary(corpus)\n",
    "# vocab_size = len(vocab)\n",
    "\n",
    "class GensimWord2Vec:\n",
    "    def __init__(self, corpus, embed_size, window_size, learning_rate, epochs):\n",
    "        self.corpus = [corpus]\n",
    "        self.embed_size = embed_size\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self):\n",
    "        self.model = Word2Vec(sentences=self.corpus,\n",
    "                              vector_size=self.embed_size,\n",
    "                              window=self.window_size,\n",
    "                              alpha=self.learning_rate,\n",
    "                              epochs=self.epochs,\n",
    "                              sg=1,  # Use skip-gram\n",
    "                              workers=1)  # Single worker for reproducibility\n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        return self.model.wv[word]\n",
    "    \n",
    "    def find_closest_word(self, word):\n",
    "        max_similarity = -1.0  # Start with the lowest possible similarity value\n",
    "        closest_word = None\n",
    "\n",
    "        if word not in self.model.wv:  # Check if the word exists in the model\n",
    "            return f\"{word} not found in model\"\n",
    "\n",
    "        word_vector = np.array(self.model.wv[word]).reshape(1, -1)\n",
    "        for w in self.model.wv.index_to_key:  # Loop through the model's vocab\n",
    "            if w == word:  # Skip the word itself\n",
    "                continue\n",
    "            w_vector = np.array(self.model.wv[w]).reshape(1, -1)\n",
    "            similarity_output = cosine_similarity(word_vector, w_vector)\n",
    "\n",
    "            # Check the type of the output\n",
    "            if np.isscalar(similarity_output):\n",
    "                similarity = similarity_output\n",
    "            else:\n",
    "                similarity = similarity_output[0][0]\n",
    "\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                closest_word = w\n",
    "\n",
    "        return closest_word\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, target, context):\n",
    "        in_vectors = self.in_embed(target)\n",
    "        out_vectors = self.out_embed(context)\n",
    "        scores = torch.matmul(in_vectors, out_vectors.t())\n",
    "        return scores\n",
    "\n",
    "    def train_model(self, corpus, vocab, window_size, learning_rate, epochs, negative_samples=5):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for idx, word in enumerate(corpus):\n",
    "                # Get the context words based on window size\n",
    "                start = max(0, idx - window_size)\n",
    "                end = min(len(corpus), idx + window_size + 1)\n",
    "                context_words = corpus[start:idx] + corpus[idx+1:end]\n",
    "\n",
    "                for context_word in context_words:\n",
    "                    target_tensor = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "                    context_tensor = torch.tensor([vocab[context_word]], dtype=torch.long)\n",
    "                    \n",
    "                    negative_context = torch.multinomial(torch.tensor(list(vocab.values()), dtype=torch.float), negative_samples, replacement=True)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    scores = self(target_tensor, torch.cat([context_tensor, negative_context]))\n",
    "                    \n",
    "                    # Create labels: 1 for the correct context word and 0 for the negative samples\n",
    "                    labels = torch.cat([torch.tensor([1.0]), torch.zeros(negative_samples)]).unsqueeze(0)\n",
    "                    \n",
    "                    loss = loss_fn(scores, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    \n",
    "    def get_word_vector(self, word, vocab):\n",
    "        word_idx = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "        return self.in_embed(word_idx).squeeze().detach().numpy()\n",
    "    \n",
    "    def find_closest_word(self, word, vocab):\n",
    "            max_similarity = -1.0  # Start with the lowest possible similarity value\n",
    "            closest_word = None\n",
    "\n",
    "            if word not in vocab:  # Check if the word exists in the vocab\n",
    "                return f\"{word} not found in vocab\"\n",
    "\n",
    "            word_idx = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "            word_vector = self.in_embed(word_idx).detach().numpy().reshape(1, -1)\n",
    "\n",
    "            for w, idx in vocab.items():  # Loop through the vocab\n",
    "                if w == word:  # Skip the word itself\n",
    "                    continue\n",
    "                w_idx = torch.tensor([idx], dtype=torch.long)\n",
    "                w_vector = self.in_embed(w_idx).detach().numpy().reshape(1, -1)\n",
    "\n",
    "                similarity_output = cosine_similarity(word_vector, w_vector)\n",
    "                \n",
    "                # Check the type of the output\n",
    "                if np.isscalar(similarity_output):\n",
    "                    similarity = similarity_output\n",
    "                else:\n",
    "                    similarity = similarity_output[0][0]\n",
    "                \n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    closest_word = w\n",
    "\n",
    "            return closest_word\n",
    "    \n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, context, window_size):\n",
    "        # Get the embeddings of the context words\n",
    "        in_vectors = self.in_embed(context)\n",
    "        \n",
    "        # Average the embeddings of the context words\n",
    "        in_vectors_avg = torch.mean(in_vectors, dim=0, keepdim=True)\n",
    "        \n",
    "        # Pass the averaged embeddings through the linear layer\n",
    "        out_logits = self.out_linear(in_vectors_avg)\n",
    "        return out_logits\n",
    "\n",
    "    def train_model(self, corpus, vocab, learning_rate, epochs, window_size=2):\n",
    "        self.out_linear = nn.Linear(self.embed_size, len(vocab)) \n",
    "        \n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for idx, word in enumerate(corpus[window_size:-window_size]):  # Leave space for context words\n",
    "                # Gather context words based on the window size\n",
    "                context_words = corpus[idx-window_size:idx] + corpus[idx+1:idx+1+window_size]\n",
    "                context_indices = [vocab[w] for w in context_words]\n",
    "                \n",
    "                # Convert to tensor\n",
    "                context = torch.tensor(context_indices, dtype=torch.long)\n",
    "                target_idx = vocab[word]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                predictions = self(context, window_size)\n",
    "                \n",
    "                loss = loss_fn(predictions, torch.tensor([target_idx]))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def get_word_vector(self, word, vocab):\n",
    "        word_idx = torch.tensor([vocab[word]], dtype=torch.long)\n",
    "        return self.in_embed(word_idx).detach().numpy()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Vocabulary builder\n",
    "def build_vocabulary(corpus):\n",
    "    vocab = {}\n",
    "    for word in corpus:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def SP_build_vocabulary(vocab):\n",
    "    # Assuming vocab is a dictionary, invert it\n",
    "    return {v: k for k, v in vocab.items()}\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = vec1.squeeze()\n",
    "    vec2 = vec2.squeeze()\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'e'\n",
    "word2 = 'e'\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "example_word = 36\n",
    "embed_size = 20\n",
    "window_size = 2\n",
    "learning_rate = 0.025\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'learning', 'fundamentals', 'data', 'science', 'statistics', 'data', 'science', 'statistics', 'hot', 'growing', 'fields', 'alternative', 'names', 'machine', 'learning', 'artificial', 'intelligence', 'big', 'data', 'etc', 'im', 'really', 'excited', 'talk', 'data', 'science', 'statistics', 'data', 'science', 'statistics', 'long', 'passions', 'mine', 'didnt', 'used', 'good', 'data', 'science', 'statistics', 'studying', 'data', 'science', 'statistics', 'long', 'time', 'got', 'better', 'better', 'became', 'data', 'science', 'statistics', 'expert', 'im', 'really', 'excited', 'talk', 'data', 'science', 'statistics', 'thanks', 'listening', 'talk', 'data', 'science', 'statistics']\n",
      "<class 'list'>\n",
      "{'today': 0, 'learning': 1, 'fundamentals': 2, 'data': 3, 'science': 4, 'statistics': 5, 'hot': 6, 'growing': 7, 'fields': 8, 'alternative': 9, 'names': 10, 'machine': 11, 'artificial': 12, 'intelligence': 13, 'big': 14, 'etc': 15, 'im': 16, 'really': 17, 'excited': 18, 'talk': 19, 'long': 20, 'passions': 21, 'mine': 22, 'didnt': 23, 'used': 24, 'good': 25, 'studying': 26, 'time': 27, 'got': 28, 'better': 29, 'became': 30, 'expert': 31, 'thanks': 32, 'listening': 33}\n",
      "<class 'dict'>\n",
      "34\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and process the data\n",
    "processor = TextProcessor('stopwords.txt', 'training_text.txt')\n",
    "\n",
    "\n",
    "corpus = processor.get_corpus()\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = build_vocabulary(corpus)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "#print corpus dtype\n",
    "print(corpus)\n",
    "print(type(corpus))\n",
    "print(vocab)\n",
    "print(type(vocab))\n",
    "print(vocab_size)\n",
    "print(type(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Piece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 90\n",
      "[['▁', 'T', 'od', 'a', 'y', '▁w', 'e', '▁wi', 'll', '▁be', '▁learning', '▁a', 'b', 'ou', 't', '▁th', 'e', '▁f', 'u', 'nd', 'ame', 'n', 'tal', 's', '▁', 'o', 'f', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', '.', '▁', 'D', 'ata', '▁', 'S', 'ci', 'ence', '▁a', 'nd', '▁statistics', '▁a', 're', '▁h', 'o', 't', '▁a', 'nd', '▁g', 'r', 'o', 'wi', 'ng', '▁', 'fi', 'el', 'd', 's', '▁wi', 'th', '▁', 'al', 'ter', 'n', 'ati', 've', '▁', 'n', 'ame', 's', '▁', 'o', 'f', '▁m', 'a', 'c', 'h', 'ine', '▁learning', ',', '▁ar', 'ti', 'fi', 'ci', 'al', '▁', 'in', 'te', 'll', 'ig', 'ence', ',', '▁b', 'ig', '▁data', ',', '▁', 'e', 't', 'c', '.', '▁I', \"'\", 'm', '▁', 're', 'a', 'll', 'y', '▁ex', 'ci', 'te', 'd', '▁t', 'o', '▁', 'tal', 'k', '▁t', 'o', '▁', 'y', 'ou', '▁a', 'b', 'ou', 't', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', '▁beca', 'use', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', '▁h', 'a', 've', '▁l', 'o', 'ng', '▁be', 'en', '▁a', '▁', 'p', 'a', 's', 's', 'i', 'on', 's', '▁', 'o', 'f', '▁m', 'ine', '.', '▁I', '▁', 'd', 'i', 'd', 'n', \"'\", 't', '▁', 'use', 'd', '▁t', 'o', '▁be', '▁', 've', 'r', 'y', '▁go', 'od', '▁a', 't', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', '▁b', 'u', 't', '▁a', 'f', 'ter', '▁s', 't', 'u', 'd', 'y', 'ing', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', '▁f', 'o', 'r', '▁a', '▁l', 'o', 'ng', '▁', 'ti', 'me', ',', '▁I', '▁go', 't', '▁be', 't', 'ter', '▁a', 'nd', '▁be', 't', 'ter', '▁a', 't', '▁', 'it', '▁', 'un', 'ti', 'l', '▁I', '▁beca', 'me', '▁a', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', '▁ex', 'p', 'er', 't', '.', '▁I', \"'\", 'm', '▁', 're', 'a', 'll', 'y', '▁ex', 'ci', 'te', 'd', '▁t', 'o', '▁', 'tal', 'k', '▁t', 'o', '▁', 'y', 'ou', '▁a', 'b', 'ou', 't', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', ',', '▁th', 'an', 'k', 's', '▁f', 'o', 'r', '▁l', 'ist', 'en', 'ing', '▁t', 'o', '▁', 'me', '▁', 'tal', 'k', '▁a', 'b', 'ou', 't', '▁data', '▁s', 'ci', 'ence', '▁a', 'nd', '▁statistics', '.']]\n",
      "<class 'list'>\n",
      "{0: '<unk>', 1: '<s>', 2: '</s>', 3: '▁', 4: '▁a', 5: 'o', 6: 't', 7: 'ci', 8: 'nd', 9: 'ence', 10: '▁statistics', 11: '▁data', 12: '▁s', 13: 's', 14: 'd', 15: 'y', 16: 'ou', 17: '▁t', 18: 'a', 19: ',', 20: '.', 21: '▁I', 22: '▁be', 23: 'n', 24: 'r', 25: 'f', 26: 'b', 27: 'k', 28: 'ter', 29: 'e', 30: 'll', 31: 'tal', 32: 'i', 33: '▁f', 34: 'ng', 35: 'u', 36: \"'\", 37: 've', 38: '▁ex', 39: 'me', 40: 'al', 41: 'te', 42: 're', 43: '▁l', 44: 'ti', 45: 'l', 46: 'm', 47: 'c', 48: 'p', 49: '▁learning', 50: 'use', 51: 'ig', 52: '▁beca', 53: 'ine', 54: '▁go', 55: '▁wi', 56: '▁m', 57: 'ame', 58: '▁th', 59: '▁b', 60: 'fi', 61: 'ing', 62: 'en', 63: '▁h', 64: 'h', 65: 'un', 66: 'od', 67: 'wi', 68: 'th', 69: '▁w', 70: 'D', 71: 'S', 72: 'T', 73: '▁g', 74: 'ata', 75: 'er', 76: 'ist', 77: 'in', 78: 'an', 79: '▁ar', 80: 'ati', 81: 'on', 82: 'it', 83: 'el', 84: '▁st', 85: 'x', 86: 'v', 87: 'w', 88: 'I', 89: 'g'}\n",
      "<class 'dict'>\n",
      "90\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# SentencePieceTokenizer\n",
    "tokenizer = SentencePieceTokenizer('training_text.txt', model_prefix='sentence_piece', vocab_size=90)\n",
    "tokenizer.train_tokenizer()\n",
    "\n",
    "# Tokenize the entire corpus\n",
    "tokenizer.tokenize_corpus()\n",
    "\n",
    "# Get tokenized corpus, vocab, and vocab_size\n",
    "corpus = tokenizer.get_tokenized_corpus()\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab = SP_build_vocabulary(vocab)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "for sentence in corpus:\n",
    "    for word in sentence:\n",
    "        if not isinstance(word, str):\n",
    "            print(f\"Non-string element found: {word}\")\n",
    "\n",
    "print(corpus)\n",
    "print(type(corpus))\n",
    "print(vocab)\n",
    "print(type(vocab))\n",
    "print(vocab_size)\n",
    "print(type(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random word selector\n",
    "import random\n",
    "example_word = random.choice(list(vocab.keys()))\n",
    "print(example_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim Model Training\n",
    "print(\"Training Gensim Model...\")\n",
    "modelGensim = GensimWord2Vec(corpus, embed_size, window_size, learning_rate, epochs)\n",
    "modelGensim.train()\n",
    "\n",
    "# Test the Gensim model (change 'example_word' to any word in your corpus)\n",
    "\n",
    "if example_word in vocab:\n",
    "    print(f\"Gensim Vector for {example_word}: \", modelGensim.get_word_vector(example_word))\n",
    "    vec1_gensim = modelGensim.get_word_vector(word1)\n",
    "    vec2_gensim = modelGensim.get_word_vector(word2)\n",
    "    similarity_gensim = cosine_similarity(vec1_gensim, vec2_gensim)\n",
    "    print(f\"Gensim cosine similarity between {word1} and {word2}: \", similarity_gensim)\n",
    "else:\n",
    "    print(f\"{example_word} not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkipGram Model Training\n",
    "print(\"\\nTraining SkipGram Model...\")\n",
    "modelSkipGram = SkipGram(vocab_size, embed_size)\n",
    "modelSkipGram.train_model(corpus, vocab, window_size, learning_rate, epochs)\n",
    "\n",
    "# Test the SkipGram model (change 'example_word' to any word in your corpus)\n",
    "if example_word in vocab:\n",
    "    print(f\"SkipGram Vector for {example_word}: \", modelSkipGram.get_word_vector(example_word, vocab))\n",
    "    vec1_skipgram = modelSkipGram.get_word_vector(word1, vocab)\n",
    "    vec2_skipgram = modelSkipGram.get_word_vector(word2, vocab)\n",
    "    similarity_skipgram = cosine_similarity(vec1_skipgram, vec2_skipgram)\n",
    "    print(f\"SkipGram cosine similarity between {word1} and {word2}: \", similarity_skipgram)\n",
    "else:\n",
    "    print(f\"{example_word} not in vocabulary.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW Model Training\n",
    "print(\"\\nTraining CBOW Model...\")\n",
    "modelCBOW = CBOW(vocab_size, embed_size)\n",
    "modelCBOW.train_model(corpus, vocab, learning_rate, epochs, window_size)\n",
    "\n",
    "\n",
    "\n",
    "# Test the CBOW model (change 'example_word' to any word in your corpus)\n",
    "if example_word in vocab:\n",
    "    print(f\"CBOW Vector for {example_word}: \", modelCBOW.get_word_vector(example_word, vocab))\n",
    "else:\n",
    "    print(f\"{example_word} not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest word to 'data' using Gensim: statistics\n",
      "Closest word to 'data' using SkipGram: thanks\n"
     ]
    }
   ],
   "source": [
    "closest_gensim = modelGensim.find_closest_word('data')\n",
    "print(f\"Closest word to 'data' using Gensim: {closest_gensim}\")\n",
    "\n",
    "closest_skipgram = modelSkipGram.find_closest_word('data', vocab)\n",
    "print(f\"Closest word to 'data' using SkipGram: {closest_skipgram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim cosine similarity between data and science:  0.9797871\n",
      "SkipGram cosine similarity between data and science:  0.25304967\n",
      "CBOW cosine similarity between data and science:  -0.015298413\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "word1 = 'data'\n",
    "word2 = 'science'\n",
    "\n",
    "if word1 in vocab and word2 in vocab:\n",
    "    # Gensim Model\n",
    "    vec1_gensim = modelGensim.get_word_vector(word1)\n",
    "    vec2_gensim = modelGensim.get_word_vector(word2)\n",
    "    similarity_gensim = cosine_similarity(vec1_gensim, vec2_gensim)\n",
    "    print(f\"Gensim cosine similarity between {word1} and {word2}: \", similarity_gensim)\n",
    "\n",
    "    # SkipGram Model\n",
    "    vec1_skipgram = modelSkipGram.get_word_vector(word1, vocab)\n",
    "    vec2_skipgram = modelSkipGram.get_word_vector(word2, vocab)\n",
    "    similarity_skipgram = cosine_similarity(vec1_skipgram, vec2_skipgram)\n",
    "    print(f\"SkipGram cosine similarity between {word1} and {word2}: \", similarity_skipgram)\n",
    "\n",
    "    # CBOW Model\n",
    "    vec1_cbow = modelCBOW.get_word_vector(word1, vocab)\n",
    "    vec2_cbow = modelCBOW.get_word_vector(word2, vocab)\n",
    "    similarity_cbow = cosine_similarity(vec1_cbow, vec2_cbow)\n",
    "    print(f\"CBOW cosine similarity between {word1} and {word2}: \", similarity_cbow)\n",
    "else:\n",
    "    print(f\"One or both words are not in vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Implementation attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity (data, science): 0.9985191838911647\n",
      "Cosine Similarity (data, really): -0.9423785399152232\n",
      "Cosine Similarity (data, science) is larger than (data, really): TRUE\n",
      "Euclidean Distance (data, science): 2.318116499762826\n",
      "Euclidean Distance (data, really): 8.161488249796113\n",
      "Euclidean Distance (data, science) is smaller than (data, really): TRUE\n",
      "[-5.11709606  2.17277004]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, corpus, embed_size, window_size, learning_rate, epochs):\n",
    "        self.corpus = corpus\n",
    "        self.embed_size = embed_size\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # Create vocabulary\n",
    "        self.words = set(corpus)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.words)}\n",
    "        self.index2word = {i: word for i, word in enumerate(self.words)}\n",
    "        self.vocab_size = len(self.words)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.input_word = np.random.randn(self.vocab_size, self.embed_size)\n",
    "\n",
    "        self.output_cotext_words = np.random.randn(self.embed_size, self.vocab_size)\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.loss = 0\n",
    "            for word_index, word in enumerate(self.corpus):\n",
    "                # We are going to predict the context words from the current word\n",
    "                # The context words are the words that fall within the window size\n",
    "                # around the current word\n",
    "                start = max(0, word_index - self.window_size)\n",
    "                end = min(word_index + self.window_size, len(self.corpus))\n",
    "                context_indices = [self.word2index[w] for w in self.corpus[start:end] if w != word]\n",
    "                \n",
    "                for context_index in context_indices:\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    input_word_embedding = self.input_word[context_index]\n",
    "          \n",
    "                    # print(self.embedding_to_word(input_word_embedding))\n",
    "                    dot_product = np.dot(input_word_embedding, self.output_cotext_words)\n",
    "\n",
    "                    softmaxed_probability = self.softmax(dot_product)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    self.loss += -np.log(softmaxed_probability[self.word2index[word]])\n",
    "                    \n",
    "                    # Backpropagation\n",
    "                    e = softmaxed_probability.copy()\n",
    "                    # Subtract 1 from the word that was actually the context word\n",
    "                    e[self.word2index[word]] -= 1\n",
    "                    \n",
    "                    #This is the gradient that shows how much the output embedding should change to minimize the loss\n",
    "                    gradient_for_output_embedding = np.outer(input_word_embedding, e)\n",
    "                    #This is the gradient that shows how much the input embedding should change to minimize the loss\n",
    "                    gradient_for_input_embedding = np.dot(self.output_cotext_words, e.T)\n",
    "                    \n",
    "                    # Update weights\n",
    "                    #Updating output embedding to take a STEP TOWARDS OPPOSITE TO THE GRADIENT\n",
    "                    self.output_cotext_words -= self.learning_rate * gradient_for_output_embedding\n",
    "\n",
    "                    #Updating input embedding ONLY FOR THE CONTEXT WORD in the context matrix\n",
    "                    self.input_word[context_index] -= self.learning_rate * gradient_for_input_embedding\n",
    "            \n",
    "            # print(f'Epoch: {epoch + 1}/{self.epochs}, Loss: {self.loss/len(self.corpus)}')\n",
    "            \n",
    "   \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / exp_x.sum(axis=0)\n",
    "    \n",
    "    def get_word_vector(self, word):\n",
    "        return self.input_word[self.word2index[word]]\n",
    "    \n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def euclidean_distance(vec1, vec2):\n",
    "    return np.linalg.norm(vec1 - vec2)\n",
    "\n",
    "\n",
    "\n",
    "#read training_text.txt and split it into words\n",
    "corpus = open(\"training_text.txt\", \"r\").read().split()\n",
    "\n",
    "\n",
    "model = Word2Vec(corpus, embed_size=2, window_size=2, learning_rate=0.01, epochs=1000)\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Cosine Similarity (data, science):\", cosine_similarity(model.get_word_vector(\"data\"), model.get_word_vector(\"science\")))\n",
    "print(\"Cosine Similarity (data, really):\", cosine_similarity(model.get_word_vector(\"data\"), model.get_word_vector(\"really\")))\n",
    "\n",
    "#compare above two cosine similarities with each other and print the smaller pair\n",
    "if cosine_similarity(model.get_word_vector(\"data\"), model.get_word_vector(\"science\")) > cosine_similarity(model.get_word_vector(\"data\"), model.get_word_vector(\"really\")):\n",
    "    print(\"Cosine Similarity (data, science) is larger than (data, really): TRUE\")\n",
    "else:\n",
    "    print(\"Cosine Similarity (data, science) is smaller than (data, really): FALSE\")\n",
    "\n",
    "\n",
    "print(\"Euclidean Distance (data, science):\", euclidean_distance(model.get_word_vector(\"data\"), model.get_word_vector(\"science\")))\n",
    "print(\"Euclidean Distance (data, really):\", euclidean_distance(model.get_word_vector(\"data\"), model.get_word_vector(\"really\")))\n",
    "\n",
    "#compare above two euclidean distances with each other and print the smaller pair\n",
    "if euclidean_distance(model.get_word_vector(\"data\"), model.get_word_vector(\"science\")) > euclidean_distance(model.get_word_vector(\"data\"), model.get_word_vector(\"really\")):\n",
    "    print(\"Euclidean Distance (data, science) is larger than (data, really): FALSE\")\n",
    "else:\n",
    "    print(\"Euclidean Distance (data, science) is smaller than (data, really): TRUE\")\n",
    "\n",
    "print(model.get_word_vector(\"science\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
